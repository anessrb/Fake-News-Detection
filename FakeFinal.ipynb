{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import resample\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Télécharger les ressources nltk nécessaires\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Charger les données à partir des fichiers CSV\n",
    "train_data = pd.read_csv(\"HAI817_Projet_train.csv\")\n",
    "test_data = pd.read_csv(\"HAI817_Projet_test.csv\")\n",
    "\n",
    "# Concaténer les données\n",
    "df = pd.concat([train_data, test_data], ignore_index=True)\n"
   ],
   "id": "9708dd62727d74d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Supprimer les colonnes non nécessaires\n",
    "df.drop(columns=['ID', 'public_id'], inplace=True)\n",
    "\n",
    "# Vérifier les valeurs manquantes et les supprimer\n",
    "df.dropna(subset=['title', 'text'], how='all', inplace=True)\n",
    "\n"
   ],
   "id": "fec63538b59be7b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['our rating'].value_counts()",
   "id": "99fe21f1f55a537e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Afficher la répartition des classes\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=df, x='our rating', order=df['our rating'].value_counts().index)\n",
    "plt.title('Répartition des classes')\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Nombre')\n",
    "plt.show()"
   ],
   "id": "ba2aa3ad23fd2e81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Définir la fonction de nettoyage de texte\n",
    "def MyCleanText(X, lowercase=True, removestopwords=False, removedigit=False, getstemmer=False, getlemmatisation=False):\n",
    "    sentence = str(X)\n",
    "    sentence = re.sub(r'[^\\w\\s]', ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence, flags=re.I)\n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    if lowercase:\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    words = [token.translate(table) for token in tokens]\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "    \n",
    "    if removedigit:\n",
    "        words = [word for word in words if not word.isdigit()]\n",
    "    \n",
    "    if removestopwords:\n",
    "        words = [word for word in words if not word in stop_words]\n",
    "    \n",
    "    if getlemmatisation:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    if getstemmer:\n",
    "        ps = PorterStemmer()\n",
    "        words = [ps.stem(word) for word in words]\n",
    "    \n",
    "    sentence = ' '.join(words)\n",
    "    return sentence\n"
   ],
   "id": "8c01e628a6e3cdf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Définir la classe de normalisation de texte\n",
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, removestopwords=False, lowercase=False, removedigit=False, getstemmer=False, getlemmatisation=False):\n",
    "        self.lowercase = lowercase\n",
    "        self.getstemmer = getstemmer\n",
    "        self.removestopwords = removestopwords\n",
    "        self.getlemmatisation = getlemmatisation\n",
    "        self.removedigit = removedigit\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        X = X.copy()\n",
    "        return [MyCleanText(text, lowercase=self.lowercase, getstemmer=self.getstemmer, removestopwords=self.removestopwords, getlemmatisation=self.getlemmatisation, removedigit=self.removedigit) for text in X]\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        return self.fit(X).transform(X)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'lowercase': self.lowercase, 'getstemmer': self.getstemmer, 'removestopwords': self.removestopwords, 'getlemmatisation': self.getlemmatisation, 'removedigit': self.removedigit}\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n"
   ],
   "id": "a28dbb428d24ef1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Définir les classifieurs et leurs hyperparamètres, ces hyperparametres ont les a trouvés grace a GridSearch en utilisant une pipeline que vous allez trouver commentée en bas.\n",
    "classifiers = {\n",
    "    'SVC': SVC(C=300, gamma='scale', kernel='linear'),\n",
    "   'LogisticRegression': LogisticRegression(C=100, penalty='l2', solver='liblinear'),\n",
    "     'GradientBoosting': GradientBoostingClassifier(learning_rate=0.2, n_estimators=300),\n",
    "    'KNN': KNeighborsClassifier(metric='euclidean', n_neighbors=7, weights='distance'),\n",
    "    'AdaBoost': AdaBoostClassifier(learning_rate=0.1, n_estimators=50),\n",
    "    'RandomForest': RandomForestClassifier(max_features='sqrt', n_estimators=100)\n",
    "}\n"
   ],
   "id": "391e1e2dd6c80c6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def execute_classifications(classifiers, train_texts, train_labels, test_texts, test_labels, labels):\n",
    "    results = []\n",
    "    for name, classifier in classifiers.items():\n",
    "        pipeline = ImbPipeline([\n",
    "            ('text_normalizer', TextNormalizer(lowercase=True, removestopwords=True)),\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('smote', SMOTE(random_state=42)),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        \n",
    "        cv_scores = cross_val_score(pipeline, train_texts, train_labels, cv=10, scoring='accuracy')\n",
    "        pipeline.fit(train_texts, train_labels)\n",
    "        test_predictions = pipeline.predict(test_texts)\n",
    "        conf_matrix = confusion_matrix(test_labels, test_predictions)\n",
    "        class_report = classification_report(test_labels, test_predictions)  # Ajout du rapport de classification\n",
    "\n",
    "        results.append({\n",
    "            'Classifier': name,\n",
    "            'CV Score Mean': cv_scores.mean(),\n",
    "            'Confusion Matrix': conf_matrix,\n",
    "            'Classification Report': class_report,  # Ajout au dictionnaire\n",
    "            'CV_Score': cv_scores\n",
    "        })\n",
    "    return results\n"
   ],
   "id": "954ff72abc6a7676",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def display_results(results, title, labels):\n",
    "    # Afficher les matrices de confusion avec labels et les rapports de classification\n",
    "    for result in results:\n",
    "        print(f\"Classification Report pour {result['Classifier']}:\\n{result['Classification Report']}\")\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.heatmap(result['Confusion Matrix'], annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=labels, yticklabels=labels)\n",
    "        plt.title(f'Matrice de confusion pour {result[\"Classifier\"]}')\n",
    "        plt.xlabel('Prédictions')\n",
    "        plt.ylabel('Étiquettes réelles')\n",
    "        plt.show()\n",
    "    \n",
    "    # Afficher les scores de validation croisée moyens\n",
    "    for result in results:\n",
    "        print(f\"Validation croisée (k=10) pour {result['Classifier']} - Accuracy: {result['CV Score Mean']}\")\n",
    "\n",
    "    # Créer un DataFrame pour les scores\n",
    "    cv_results = pd.DataFrame({\n",
    "        'Classifier': [],\n",
    "        'CV Score': []\n",
    "    })\n",
    "    for result in results:\n",
    "        temp_df = pd.DataFrame({'Classifier': [result['Classifier']] * len(result['CV_Score']), 'CV Score': result['CV_Score']})\n",
    "        cv_results = pd.concat([cv_results, temp_df], ignore_index=True)\n",
    "    \n",
    "    # Créer un boxplot pour comparer les classifieurs\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sns.boxplot(x='Classifier', y='CV Score', data=cv_results)\n",
    "    plt.title(f'Comparaison des Classifieurs - {title}')\n",
    "    plt.xlabel('Classifieur')\n",
    "    plt.ylabel('Score de Validation Croisée')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n"
   ],
   "id": "b535ad41d8b990d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "CLASSIFICATION 1 : TRUE VS FALSE",
   "id": "85d9f9d70e08ce92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Classification 1 : TRUE vs FALSE\n",
    "df1 = df[df['our rating'].isin(['true', 'false'])].copy()\n",
    "df1['our rating'] = df1['our rating'].map({'true': 1, 'false': 0})\n",
    "majority = df1[df1['our rating'] == 0]\n",
    "minority = df1[df1['our rating'] == 1]\n",
    "majority_downsampled = resample(majority, replace=False, n_samples=len(minority), random_state=42)\n",
    "balanced_df1 = pd.concat([majority_downsampled, minority])\n",
    "train_df1, test_df1 = train_test_split(balanced_df1, test_size=0.2, random_state=42)\n",
    "train_titles_texts1 = train_df1['title'] + \" \" + train_df1['text']\n",
    "test_titles_texts1 = test_df1['title'] + \" \" + test_df1['text']\n",
    "results1 = execute_classifications(classifiers, train_titles_texts1, train_df1['our rating'], test_titles_texts1, test_df1['our rating'], ['False', 'True'])\n",
    "display_results(results1, 'True vs False', ['False', 'True'])"
   ],
   "id": "8f6a174d89d93022",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "PICKLE DU MODELE SVC",
   "id": "2ec962422a0af36c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "# Préparer le pipeline avec les paramètres optimaux pour SVC\n",
    "pipeline = Pipeline([\n",
    "    ('text_normalizer', TextNormalizer(lowercase=True, removestopwords=True)),\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', SVC(C=300, gamma='scale', kernel='linear'))\n",
    "])\n",
    "\n",
    "# Entraîner le modèle\n",
    "pipeline.fit(train_titles_texts1, train_df1['our rating'])\n",
    "\n",
    "# Sauvegarder le modèle avec pickle\n",
    "with open('svc_model.pkl', 'wb') as file:\n",
    "    pickle.dump(pipeline, file)\n"
   ],
   "id": "7ef8b349789f84a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3252bcec17e26c62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Explain like i was 5 YO",
   "id": "53d3e9f55a6def79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "df1 = df[df['our rating'].isin(['true', 'false'])].copy()\n",
    "df1['our rating'] = df1['our rating'].map({'true': 1, 'false': 0})\n",
    "df1['title'] = df1['title'].fillna('')\n",
    "df1['text'] = df1['text'].fillna('')\n",
    "\n",
    "df1_subset = df1.sample(frac=0.10, random_state=42)\n",
    "\n",
    "\n",
    "train_titles_texts1 = df1_subset['title'] + \" \" + df1_subset['text']\n",
    "train_labels1 = df1_subset['our rating']\n",
    "\n",
    "pipeline = ImbPipeline([\n",
    "    ('text_normalizer', TextNormalizer(lowercase=True, removestopwords=True)),\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', SVC(C=300, gamma='scale', kernel='linear', probability=True))\n",
    "])\n",
    "\n",
    "pipeline.fit(train_titles_texts1, train_labels1)\n",
    "\n",
    "# Feature importance with ELI5\n",
    "tfidf_transformed = pipeline.named_steps['tfidf'].transform(train_titles_texts1).toarray()\n",
    "perm = PermutationImportance(pipeline.named_steps['classifier'], random_state=42).fit(tfidf_transformed, train_labels1)\n",
    "eli5.show_weights(perm, feature_names=pipeline.named_steps['tfidf'].get_feature_names_out())\n",
    "\n",
    "# Choose a single instance from the test set to explain\n",
    "sample_text = train_titles_texts1.iloc[0]\n",
    "sample_vectorized = pipeline.named_steps['tfidf'].transform([sample_text]).toarray()\n",
    "\n",
    "# Explain the prediction for the single instance\n",
    "explanation = eli5.explain_prediction(pipeline.named_steps['classifier'], sample_vectorized[0], feature_names=pipeline.named_steps['tfidf'].get_feature_names_out())\n",
    "eli5.show_prediction(pipeline.named_steps['classifier'], sample_vectorized[0], feature_names=pipeline.named_steps['tfidf'].get_feature_names_out())\n",
    "\n",
    "# Display the explanations\n",
    "print(f\"Prediction for the sample text: {'True' if pipeline.named_steps['classifier'].predict(sample_vectorized)[0] == 1 else 'False'}\")\n",
    "display(eli5.show_prediction(pipeline.named_steps['classifier'], sample_vectorized[0], feature_names=pipeline.named_steps['tfidf'].get_feature_names_out()))\n"
   ],
   "id": "e06c4f6f21dd73bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "CLASSIFICATION 2 : TRUE AND FALSE VS OTHER",
   "id": "ae2b35c050eb3df8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Classification 2 : TRUE and FALSE vs OTHER\n",
    "df2 = df[df['our rating'].isin(['true', 'false', 'other'])].copy()\n",
    "df2['our rating'] = df2['our rating'].apply(lambda x: 0 if x in ['true', 'false'] else 1)\n",
    "majority = df2[df2['our rating'] == 0]\n",
    "minority = df2[df2['our rating'] == 1]\n",
    "majority_downsampled = resample(majority, replace=False, n_samples=len(minority), random_state=42)\n",
    "balanced_df2 = pd.concat([majority_downsampled, minority])\n",
    "train_df2, test_df2 = train_test_split(balanced_df2, test_size=0.2, random_state=42)\n",
    "train_titles_texts2 = train_df2['title'] + \" \" + train_df2['text']\n",
    "test_titles_texts2 = test_df2['title'] + \" \" + test_df2['text']\n",
    "results2 = execute_classifications(classifiers, train_titles_texts2, train_df2['our rating'], test_titles_texts2, test_df2['our rating'], ['True/False', 'Other'])\n",
    "display_results(results2, 'True/False vs Other', ['True/False', 'Other'])\n"
   ],
   "id": "1a5f8e518346a699",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "CLASSIFICATION 3 : TRUE VS FALSE VS OTHER VS MIXTURE",
   "id": "88880cfc9a408dff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Classification 3 : TRUE vs FALSE vs OTHER vs MIXTURE\n",
    "df3 = df[df['our rating'].isin(['true', 'false', 'mixture', 'other'])].copy()\n",
    "rating_map = {'true': 0, 'false': 1, 'mixture': 2, 'other': 3}\n",
    "df3['our rating'] = df3['our rating'].map(rating_map)\n",
    "classes = df3['our rating'].unique()\n",
    "dfs = [df3[df3['our rating'] == cls] for cls in classes]\n",
    "majority_size = min([len(cls_df) for cls_df in dfs])\n",
    "dfs_downsampled = [resample(cls_df, replace=False, n_samples=majority_size, random_state=42) for cls_df in dfs]\n",
    "balanced_df3 = pd.concat(dfs_downsampled)\n",
    "train_df3, test_df3 = train_test_split(balanced_df3, test_size=0.2, random_state=42)\n",
    "train_titles_texts3 = train_df3['title'] + \" \" + train_df3['text']\n",
    "test_titles_texts3 = test_df3['title'] + \" \" + test_df3['text']\n",
    "results3 = execute_classifications(classifiers, train_titles_texts3, train_df3['our rating'], test_titles_texts3, test_df3['our rating'], ['True', 'False', 'Mixture', 'Other'])\n",
    "display_results(results3, 'True vs False vs Mixture vs Other', ['True', 'False', 'Mixture', 'Other'])"
   ],
   "id": "934afb1373291f7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "18bfdf9e2c9a2043",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6210c67a61e64323",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "dace59fe7bafb0c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9eabcd7a0a81c35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "89dcf306db3b0638",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "98b0b6f87d3e9d83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a504f0293c9b06e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LES PIPELINES AVEC GRIDSEARCH POUR TROUVER LES MEILLEURS HYPERPARAMETRES ET MEILLEURS PRETRAITEMENTS, LANCEZ TOUT LE BLOC, CELA PREND DES HEURES, DES FOIS DES JOURS...",
   "id": "cb540e9e5cf19e5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "PIPELINE CLASSIFICATION 1 ",
   "id": "b8af8d978b7a6dfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install xgboost imbalanced-learn\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Télécharger les ressources nltk nécessaires\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Charger les données à partir des fichiers CSV\n",
    "train_data = pd.read_csv(\"HAI817_Projet_train.csv\")\n",
    "test_data = pd.read_csv(\"HAI817_Projet_test.csv\")\n",
    "\n",
    "# Concaténer les données\n",
    "df = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "# Supprimer les colonnes non nécessaires\n",
    "df.drop(columns=['ID', 'public_id'], inplace=True)\n",
    "\n",
    "# Vérifier les valeurs manquantes et les supprimer\n",
    "df.dropna(subset=['title', 'text'], how='all', inplace=True)\n",
    "\n",
    "# Filtrer les lignes où 'our rating' est 'true' ou 'false'\n",
    "filtered_df = df[df['our rating'].isin(['true', 'false'])].copy()\n",
    "\n",
    "# Convertir 'true' en 1 et 'false' en 0\n",
    "filtered_df['our rating'] = filtered_df['our rating'].map({'true': 1, 'false': 0})\n",
    "\n",
    "# Downsample la classe majoritaire\n",
    "majority = filtered_df[filtered_df['our rating'] == 0]\n",
    "minority = filtered_df[filtered_df['our rating'] == 1]\n",
    "\n",
    "# Downsample majority class\n",
    "majority_downsampled = resample(majority, \n",
    "                              replace=False,    # échantillon sans remplacement\n",
    "                              n_samples=len(minority),  # pour correspondre au nombre de minorités\n",
    "                              random_state=42)  # pour la reproductibilité\n",
    "\n",
    "# Concaténer les classes minoritaires et majoritaires rééchantillonnées\n",
    "balanced_df = pd.concat([majority_downsampled, minority])\n",
    "\n",
    "# Séparer les données en jeu d'entraînement et de test\n",
    "train_df, test_df = train_test_split(balanced_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Préparer les ensembles d'entraînement et de test\n",
    "train_titles_texts = train_df['title'] + \" \" + train_df['text']\n",
    "test_titles_texts = test_df['title'] + \" \" + test_df['text']\n",
    "\n",
    "# Définir la fonction de nettoyage de texte\n",
    "def MyCleanText(X, lowercase=True, removestopwords=False, removedigit=False, getstemmer=False, getlemmatisation=False):\n",
    "    sentence = str(X)\n",
    "    sentence = re.sub(r'[^\\w\\s]', ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence, flags=re.I)\n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    if lowercase:\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    words = [token.translate(table) for token in tokens]\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "    \n",
    "    if removedigit:\n",
    "        words = [word for word in words if not word.isdigit()]\n",
    "    \n",
    "    if removestopwords:\n",
    "        words = [word for word in words if not word in stop_words]\n",
    "    \n",
    "    if getlemmatisation:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    if getstemmer:\n",
    "        ps = PorterStemmer()\n",
    "        words = [ps.stem(word) for word in words]\n",
    "    \n",
    "    sentence = ' '.join(words)\n",
    "    return sentence\n",
    "\n",
    "# Définir la classe de normalisation de texte\n",
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, removestopwords=False, lowercase=False, removedigit=False, getstemmer=False, getlemmatisation=False):\n",
    "        self.lowercase = lowercase\n",
    "        self.getstemmer = getstemmer\n",
    "        self.removestopwords = removestopwords\n",
    "        self.getlemmatisation = getlemmatisation\n",
    "        self.removedigit = removedigit\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        X = X.copy()\n",
    "        return [MyCleanText(text, lowercase=self.lowercase, getstemmer=self.getstemmer, removestopwords=self.removestopwords, getlemmatisation=self.getlemmatisation, removedigit=self.removedigit) for text in X]\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        return self.fit(X).transform(X)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'lowercase': self.lowercase, 'getstemmer': self.getstemmer, 'removestopwords': self.removestopwords, 'getlemmatisation': self.getlemmatisation, 'removedigit': self.removedigit}\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "# Préparer les configurations de prétraitement\n",
    "preprocessing_options = [\n",
    "    {'lowercase': True, 'removestopwords': True, 'removedigit': False, 'getlemmatisation': False},\n",
    "    {'lowercase': True, 'removestopwords': True, 'removedigit': True, 'getlemmatisation': True},\n",
    "    {'lowercase': True, 'removestopwords': True, 'removedigit': False, 'getlemmatisation': True},\n",
    "    {'lowercase': False, 'removestopwords': True, 'removedigit': True, 'getlemmatisation': True},\n",
    "    {'lowercase': True, 'removestopwords': False, 'removedigit': True, 'getlemmatisation': True},\n",
    "    {'lowercase': True, 'removestopwords': True, 'removedigit': True, 'getlemmatisation': True}\n",
    "]\n",
    "# Définir les classifieurs et leurs grilles de paramètres\n",
    "classifiers = {\n",
    "    'SVC': SVC(),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'GradientBoosting': GradientBoostingClassifier(),\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'SVC': {\n",
    "        'classifier__C': [0.1, 1, 10, 100, 300],\n",
    "        'classifier__gamma': ['scale', 'auto'],\n",
    "        'classifier__kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'classifier__C': [0.1, 1, 10, 100],\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__solver': ['liblinear']\n",
    "    },\n",
    "    'KNN': {\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9],\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__metric': ['euclidean', 'manhattan']\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 1.0]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'classifier__max_depth': [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for preprocess_params in preprocessing_options:\n",
    "    preprocess_name = '_'.join([k for k, v in preprocess_params.items() if v])\n",
    "    for classifier_name, classifier in classifiers.items():\n",
    "        pipeline = ImbPipeline([\n",
    "            ('text_normalizer', TextNormalizer(**preprocess_params)),\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('smote', SMOTE(random_state=42)),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        \n",
    "        param_grid = param_grids[classifier_name]\n",
    "        \n",
    "        grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='accuracy')\n",
    "        grid_search.fit(train_titles_texts, train_df['our rating'])\n",
    "        \n",
    "        mean_score = grid_search.best_score_\n",
    "        \n",
    "        results.append({\n",
    "            'Preprocessing': preprocess_name,\n",
    "            'Classifier': classifier_name,\n",
    "            'Accuracy': mean_score,\n",
    "            'Best Params': grid_search.best_params_\n",
    "        })\n",
    "\n",
    "# Afficher les résultats\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Sauvegarder les résultats dans un fichier CSV\n",
    "results_df.to_csv(\"results_with_smote.csv\", index=False)\n"
   ],
   "id": "90e3c474ce96b89c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "PIPELINE CLASSIFICATION 2",
   "id": "51e7b7be695fdd8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install xgboost imbalanced-learn\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Télécharger les ressources nltk nécessaires\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Charger les données à partir des fichiers CSV\n",
    "train_data = pd.read_csv(\"HAI817_Projet_train.csv\")\n",
    "test_data = pd.read_csv(\"HAI817_Projet_test.csv\")\n",
    "\n",
    "# Concaténer les données\n",
    "df = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "# Supprimer les colonnes non nécessaires\n",
    "df.drop(columns=['ID', 'public_id'], inplace=True)\n",
    "\n",
    "# Vérifier les valeurs manquantes et les supprimer\n",
    "df.dropna(subset=['title', 'text'], how='all', inplace=True)\n",
    "\n",
    "# Filtrer les lignes où 'our rating' est 'true', 'false' ou 'other'\n",
    "df = df[df['our rating'].isin(['true', 'false', 'other'])].copy()\n",
    "df['our rating'] = df['our rating'].apply(lambda x: 0 if x in ['true', 'false'] else 1)\n",
    "\n",
    "# Downsample la classe majoritaire\n",
    "majority = df[df['our rating'] == 0]\n",
    "minority = df[df['our rating'] == 1]\n",
    "\n",
    "# Downsample majority class\n",
    "majority_downsampled = resample(majority, \n",
    "                              replace=False,    # échantillon sans remplacement\n",
    "                              n_samples=len(minority),  # pour correspondre au nombre de minorités\n",
    "                              random_state=42)  # pour la reproductibilité\n",
    "\n",
    "# Concaténer les classes minoritaires et majoritaires rééchantillonnées\n",
    "balanced_df = pd.concat([majority_downsampled, minority])\n",
    "\n",
    "# Séparer les données en jeu d'entraînement et de test\n",
    "train_df, test_df = train_test_split(balanced_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Préparer les ensembles d'entraînement et de test\n",
    "train_titles_texts = train_df['title'] + \" \" + train_df['text']\n",
    "test_titles_texts = test_df['title'] + \" \" + test_df['text']\n",
    "\n",
    "# Définir la fonction de nettoyage de texte\n",
    "def MyCleanText(X, lowercase=True, removestopwords=False, removedigit=False, getstemmer=False, getlemmatisation=False):\n",
    "    sentence = str(X)\n",
    "    sentence = re.sub(r'[^\\w\\s]', ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence, flags=re.I)\n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    if lowercase:\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    words = [token.translate(table) for token in tokens]\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "    \n",
    "    if removedigit:\n",
    "        words = [word for word in words if not word.isdigit()]\n",
    "    \n",
    "    if removestopwords:\n",
    "        words = [word for word in words if not word in stop_words]\n",
    "    \n",
    "    if getlemmatisation:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    if getstemmer:\n",
    "        ps = PorterStemmer()\n",
    "        words = [ps.stem(word) for word in words]\n",
    "    \n",
    "    sentence = ' '.join(words)\n",
    "    return sentence\n",
    "\n",
    "# Définir la classe de normalisation de texte\n",
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, removestopwords=False, lowercase=False, removedigit=False, getstemmer=False, getlemmatisation=False):\n",
    "        self.lowercase = lowercase\n",
    "        self.getstemmer = getstemmer\n",
    "        self.removestopwords = removestopwords\n",
    "        self.getlemmatisation = getlemmatisation\n",
    "        self.removedigit = removedigit\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        X = X.copy()\n",
    "        return [MyCleanText(text, lowercase=self.lowercase, getstemmer=self.getstemmer, removestopwords=self.removestopwords, getlemmatisation=self.getlemmatisation, removedigit=self.removedigit) for text in X]\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        return self.fit(X).transform(X)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'lowercase': self.lowercase, 'getstemmer': self.getstemmer, 'removestopwords': self.removestopwords, 'getlemmatisation': self.getlemmatisation, 'removedigit': self.removedigit}\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "# Préparer les configurations de prétraitement\n",
    "preprocessing_options = [\n",
    "    {'lowercase': True, 'removestopwords': True, 'removedigit': False, 'getlemmatisation': False},\n",
    "    {'lowercase': True, 'removestopwords': True, 'removedigit': True, 'getlemmatisation': True},\n",
    "    {'lowercase': True, 'removestopwords': True, 'removedigit': False, 'getlemmatisation': True},\n",
    "    {'lowercase': False, 'removestopwords': True, 'removedigit': True, 'getlemmatisation': True},\n",
    "    {'lowercase': True, 'removestopwords': False, 'removedigit': True, 'getlemmatisation': True},\n",
    "    {'lowercase': True, 'removestopwords': True, 'removedigit': True, 'getlemmatisation': True}\n",
    "]\n",
    "# Définir les classifieurs et leurs grilles de paramètres\n",
    "classifiers = {\n",
    "    'SVC': SVC(),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'GradientBoosting': GradientBoostingClassifier(),\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'SVC': {\n",
    "        'classifier__C': [0.1, 1, 10, 100, 300],\n",
    "        'classifier__gamma': ['scale', 'auto'],\n",
    "        'classifier__kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'classifier__C': [0.1, 1, 10, 100],\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__solver': ['liblinear']\n",
    "    },\n",
    "    'KNN': {\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9],\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__metric': ['euclidean', 'manhattan']\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 1.0]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'classifier__max_depth': [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for preprocess_params in preprocessing_options:\n",
    "    preprocess_name = '_'.join([k for k, v in preprocess_params.items() if v])\n",
    "    for classifier_name, classifier in classifiers.items():\n",
    "        pipeline = ImbPipeline([\n",
    "            ('text_normalizer', TextNormalizer(**preprocess_params)),\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('smote', SMOTE(random_state=42)),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        \n",
    "        param_grid = param_grids[classifier_name]\n",
    "        \n",
    "        grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='accuracy')\n",
    "        grid_search.fit(train_titles_texts, train_df['our rating'])\n",
    "        \n",
    "        mean_score = grid_search.best_score_\n",
    "        \n",
    "        results.append({\n",
    "            'Preprocessing': preprocess_name,\n",
    "            'Classifier': classifier_name,\n",
    "            'Accuracy': mean_score,\n",
    "            'Best Params': grid_search.best_params_\n",
    "        })\n",
    "\n",
    "# Afficher les résultats\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Sauvegarder les résultats dans un fichier CSV\n",
    "results_df.to_csv(\"final/2emeclassification_results_with_smote.csv\", index=False)"
   ],
   "id": "d8d9a5ec3fe36496",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "PIPELINE CLASSIFICATION 3",
   "id": "8b576013395e734d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install xgboost imbalanced-learn\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Télécharger les ressources nltk nécessaires\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Charger les données à partir des fichiers CSV\n",
    "train_data = pd.read_csv(\"HAI817_Projet_train.csv\")\n",
    "test_data = pd.read_csv(\"HAI817_Projet_test.csv\")\n",
    "\n",
    "# Concaténer les données\n",
    "df = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "# Supprimer les colonnes non nécessaires\n",
    "df.drop(columns=['ID', 'public_id'], inplace=True)\n",
    "\n",
    "# Vérifier les valeurs manquantes et les supprimer\n",
    "df.dropna(subset=['title', 'text'], how='all', inplace=True)\n",
    "\n",
    "# Filtrer les lignes où 'our rating' est 'true', 'false', 'mixture', ou 'other'\n",
    "df = df[df['our rating'].isin(['true', 'false', 'mixture', 'other'])].copy()\n",
    "\n",
    "# Mapper les valeurs de 'our rating' à des entiers\n",
    "rating_map = {'true': 0, 'false': 1, 'mixture': 2, 'other': 3}\n",
    "df['our rating'] = df['our rating'].map(rating_map)\n",
    "\n",
    "# Upsample les classes minoritaires\n",
    "classes = df['our rating'].unique()\n",
    "dfs = [df[df['our rating'] == cls] for cls in classes]\n",
    "majority_size = max([len(cls_df) for cls_df in dfs])\n",
    "dfs_upsampled = [resample(cls_df, replace=True, n_samples=majority_size, random_state=42) for cls_df in dfs]\n",
    "balanced_df = pd.concat(dfs_upsampled)\n",
    "\n",
    "# Définir la fonction de nettoyage de texte\n",
    "def MyCleanText(X, lowercase=True, removestopwords=False, removedigit=False, getstemmer=False, getlemmatisation=False):\n",
    "    sentence = str(X)\n",
    "    sentence = re.sub(r'[^\\w\\s]', ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence, flags=re.I)\n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    if lowercase:\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    words = [token.translate(table) for token in tokens]\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "    \n",
    "    if removedigit:\n",
    "        words = [word for word in words if not word.isdigit()]\n",
    "    \n",
    "    if removestopwords:\n",
    "        words = [word for word in words if not word in stop_words]\n",
    "    \n",
    "    if getlemmatisation:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    if getstemmer:\n",
    "        ps = PorterStemmer()\n",
    "        words = [ps.stem(word) for word in words]\n",
    "    \n",
    "    sentence = ' '.join(words)\n",
    "    return sentence\n",
    "\n",
    "# Définir la classe de normalisation de texte\n",
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, removestopwords=False, lowercase=False, removedigit=False, getstemmer=False, getlemmatisation=False):\n",
    "        self.lowercase = lowercase\n",
    "        self.getstemmer = getstemmer\n",
    "        self.removestopwords = removestopwords\n",
    "        self.getlemmatisation = getlemmatisation\n",
    "        self.removedigit = removedigit\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        X = X.copy()\n",
    "        return [MyCleanText(text, lowercase=self.lowercase, getstemmer=self.getstemmer, removestopwords=self.removestopwords, getlemmatisation=self.getlemmatisation, removedigit=self.removedigit) for text in X]\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        return self.fit(X).transform(X)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'lowercase': self.lowercase, 'getstemmer': self.getstemmer, 'removestopwords': self.removestopwords, 'getlemmatisation': self.getlemmatisation, 'removedigit': self.removedigit}\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "titles_texts = balanced_df['title'] + \" \" + balanced_df['text']\n",
    "\n",
    "# Configurations pour les tests\n",
    "datasets = {\n",
    "    'Title_Text': titles_texts\n",
    "}\n",
    "\n",
    "preprocessing_options = [\n",
    "    {'lowercase': True, 'removestopwords': True, 'removedigit': False, 'getlemmatisation': False},\n",
    "    {'lowercase': True, 'removestopwords': True, 'removedigit': True, 'getlemmatisation': True},\n",
    "    {'lowercase': True, 'removestopwords': True, 'removedigit': False, 'getlemmatisation': True},\n",
    "    {'lowercase': False, 'removestopwords': True, 'removedigit': True, 'getlemmatisation': True},\n",
    "    {'lowercase': True, 'removestopwords': False, 'removedigit': True, 'getlemmatisation': True},\n",
    "    {'lowercase': True, 'removestopwords': True, 'removedigit': True, 'getlemmatisation': True}\n",
    "]\n",
    "\n",
    "# Définir les classifieurs et leurs grilles de paramètres\n",
    "classifiers = {\n",
    "    'SVC': SVC(),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'GradientBoosting': GradientBoostingClassifier(),\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'SVC': {\n",
    "        'classifier__C': [0.1, 1, 10, 100, 300],\n",
    "        'classifier__gamma': ['scale', 'auto'],\n",
    "        'classifier__kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'classifier__C': [0.1, 1, 10, 100],\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__solver': ['liblinear']\n",
    "    },\n",
    "    'KNN': {\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9],\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__metric': ['euclidean', 'manhattan']\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 1.0]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'classifier__max_depth': [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    for preprocess_params in preprocessing_options:\n",
    "        preprocess_name = '_'.join([k for k, v in preprocess_params.items() if v])\n",
    "        for classifier_name, classifier in classifiers.items():\n",
    "            pipeline = ImbPipeline([\n",
    "                ('text_normalizer', TextNormalizer(**preprocess_params)),\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('smote', SMOTE(random_state=42)),\n",
    "                ('classifier', classifier)\n",
    "            ])\n",
    "            \n",
    "            param_grid = param_grids[classifier_name]\n",
    "            \n",
    "            grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='accuracy')\n",
    "            grid_search.fit(dataset, balanced_df['our rating'])\n",
    "            \n",
    "            mean_score = grid_search.best_score_\n",
    "            \n",
    "            results.append({\n",
    "                'Preprocessing': preprocess_name,\n",
    "                'Classifier': classifier_name,\n",
    "                'Accuracy': mean_score,\n",
    "                'Best Params': grid_search.best_params_\n",
    "            })\n",
    "\n",
    "# Afficher les résultats\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Sauvegarder les résultats dans un fichier CSV\n",
    "results_df.to_csv(\"final/3meclassification_results_with_smote.csv\", index=False)"
   ],
   "id": "1fc41d32837ff483",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
