

from sklearn.base import BaseEstimator, TransformerMixin
import re
import string
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
from nltk import word_tokenize

# Télécharger les ressources nltk nécessaires
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')
stop_words = set(stopwords.words('english'))

class TextNormalizer(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.wnl = WordNetLemmatizer()
        self.ps = PorterStemmer()
    
    def normalize(self, document):
        document = document.lower()
        document = re.sub(r'\d+', '', document)
        document = document.translate(str.maketrans('', '', string.punctuation))
        tokens = word_tokenize(document)
        tokens = [word for word in tokens if word not in stop_words]
        tokens = [self.wnl.lemmatize(word) for word in tokens]
        return ' '.join(tokens)
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        return [self.normalize(doc) for doc in X]
